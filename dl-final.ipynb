{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tokenizers, math, pickle\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as kfold\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold as stratkfold\nfrom transformers import *","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T02:02:29.539040Z","iopub.execute_input":"2021-12-21T02:02:29.539411Z","iopub.status.idle":"2021-12-21T02:02:35.864842Z","shell.execute_reply.started":"2021-12-21T02:02:29.539369Z","shell.execute_reply":"2021-12-21T02:02:35.863866Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Constants\nLEN = 96\nEPOCHS = 3\nBATCH_SIZE = 32\nSEED = 88888\nMAP_SENTI_TO_ID = {'positive': 1313,\n                   'negative': 2430,\n                   'neutral': 7974}\n\npath_vocab = '../input/tf-roberta/vocab-roberta-base.json'\npath_merge = '../input/tf-roberta/merges-roberta-base.txt'\npath_pretrained = '../input/tf-roberta/pretrained-roberta-base.h5'\npath_train = '../input/tweet-sentiment-extraction/train.csv'\npath_test = '../input/tweet-sentiment-extraction/test.csv'\npath_config = '../input/tf-roberta/config-roberta-base.json'","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:02:35.867944Z","iopub.execute_input":"2021-12-21T02:02:35.868712Z","iopub.status.idle":"2021-12-21T02:02:35.879048Z","shell.execute_reply.started":"2021-12-21T02:02:35.868666Z","shell.execute_reply":"2021-12-21T02:02:35.878234Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Utils\ndef jaccard_idx(str1, str2):\n    set1 = set(str1.lower().split())\n    set2 = set(str2.lower().split())\n    if (len(set1)==0) & (len(set2)==0):\n        return 0.5\n    else:\n        set3 = set1.intersection(set2)\n        return float(len(set3)) / (len(set1) + len(set2) - len(set3))\n\ndef dump_data(mod, path):\n    wei = mod.get_weights()\n    with open(path, 'wb') as fd:\n        pickle.dump(wei, fd)\n\ndef load_data(mod, path):\n    with open(path, 'rb') as fd:\n        wei = pickle.load(fd)\n    mod.set_weights(wei)\n    return mod\n\ndef cal_loss(yt, yhat):\n    pos = tf.shape(yhat)[1]\n    yt = yt[:, :pos]\n    ret = tf.keras.losses.categorical_crossentropy(yt, yhat, from_logits=False, label_smoothing=0.1)\n    ret = tf.reduce_mean(ret)\n    return ret","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:02:35.880329Z","iopub.execute_input":"2021-12-21T02:02:35.880730Z","iopub.status.idle":"2021-12-21T02:02:35.893214Z","shell.execute_reply.started":"2021-12-21T02:02:35.880691Z","shell.execute_reply":"2021-12-21T02:02:35.892249Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Initialize\ntrain = pd.read_csv(path_train).fillna('')\ntest = pd.read_csv(path_test).fillna('')\nconfig = RobertaConfig.from_pretrained(path_config)\ntknzer = tokenizers.ByteLevelBPETokenizer(vocab_file=path_vocab, merges_file=path_merge, lowercase=True, add_prefix_space=True)\nstrat = stratkfold(n_splits=5, shuffle=True, random_state=SEED)\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nin_ids = np.ones((train.shape[0],LEN),dtype='int32')\nin_ids_t = np.ones((test.shape[0],LEN),dtype='int32')\nmask = np.zeros((train.shape[0],LEN),dtype='int32')\nmask_t = np.zeros((test.shape[0],LEN),dtype='int32')\ntypes = np.zeros((train.shape[0],LEN),dtype='int32')\ntypes_t = np.zeros((test.shape[0],LEN),dtype='int32')\nstart = np.zeros((train.shape[0],LEN),dtype='int32')\nend = np.zeros((train.shape[0],LEN),dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:02:35.894856Z","iopub.execute_input":"2021-12-21T02:02:35.895441Z","iopub.status.idle":"2021-12-21T02:02:36.182461Z","shell.execute_reply.started":"2021-12-21T02:02:35.895403Z","shell.execute_reply":"2021-12-21T02:02:36.181668Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def convertTrainData():\n    for cur in range(train.shape[0]):\n        str1 = \" \"+\" \".join(train.loc[cur,'text'].split())\n        str2 = \" \".join(train.loc[cur,'selected_text'].split())\n\n        vec = np.zeros((len(str1)))\n        pos = str1.find(str2)\n        vec[pos:pos+len(str2)]=1\n\n        if str1[pos-1]==' ': vec[pos-1] = 1\n        enc = tknzer.encode(str1)\n\n        gather = []\n        pos=0\n        for i in enc.ids:\n            w = tknzer.decode([i])\n            gather.append((pos, pos+len(w)))\n            pos += len(w)\n\n\n        tokens = []\n        for i,(a,b) in enumerate(gather):\n            sm = np.sum(vec[a:b])\n            if sm>0:\n                tokens.append(i)\n\n        stok = MAP_SENTI_TO_ID[train.loc[cur, 'sentiment']]\n        in_ids[cur, :len(enc.ids) + 3] = [0, stok] + enc.ids + [2]\n        mask[cur, :len(enc.ids) + 3] = 1\n        if len(tokens)>0:\n            start[cur, tokens[0] + 2] = 1\n            end[cur, tokens[-1] + 2] = 1\n\ndef convertTestData():\n    for cur in range(test.shape[0]):\n        str1 = \" \"+\" \".join(test.loc[cur,'text'].split())\n        enc = tknzer.encode(str1)\n        stok = MAP_SENTI_TO_ID[test.loc[cur,'sentiment']]\n        in_ids_t[cur,:len(enc.ids)+3] = [0, stok] + enc.ids + [2]\n        mask_t[cur,:len(enc.ids)+3] = 1\n\ndef build_model():\n    _ids = tf.keras.layers.Input((LEN,), dtype=tf.int32)\n    _mask = tf.keras.layers.Input((LEN,), dtype=tf.int32)\n    _types = tf.keras.layers.Input((LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(_ids, 1), tf.int32)\n\n    cur_len = tf.reduce_max(LEN - tf.reduce_sum(padding, -1))\n    new_ids = _ids[:, :cur_len]\n    new_mask = _mask[:, :cur_len]\n    new_types = _types[:, :cur_len]\n\n\n    x_val = TFRobertaModel.from_pretrained(path_pretrained, config=config)(\n        new_ids,\n        attention_mask=new_mask,\n        token_type_ids=new_types\n    )\n    \n    x_val1 = tf.keras.layers.Dropout(0.1)(x_val[0])\n    x_val1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x_val1)\n    x_val1 = tf.keras.layers.LeakyReLU()(x_val1)\n    x_val1 = tf.keras.layers.Dense(1)(x_val1)\n    x_val1 = tf.keras.layers.Flatten()(x_val1)\n    x_val1 = tf.keras.layers.Activation('softmax')(x_val1)\n    \n    x_val2 = tf.keras.layers.Dropout(0.1)(x_val[0])\n    x_val2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x_val2)\n    x_val2 = tf.keras.layers.LeakyReLU()(x_val2)\n    x_val2 = tf.keras.layers.Dense(1)(x_val2)\n    x_val2 = tf.keras.layers.Flatten()(x_val2)\n    x_val2 = tf.keras.layers.Activation('softmax')(x_val2)\n\n    new_model = tf.keras.models.Model(inputs=[_ids, _mask, _types], outputs=[x_val1,x_val2])\n    new_model.compile(loss=cal_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5))\n    new_pad_model = tf.keras.models.Model(inputs=[_ids, _mask, _types], outputs=[tf.pad(x_val1, [[0, 0], [0, LEN - cur_len]], constant_values=0.),\n                                                                                 tf.pad(x_val2, [[0, 0], [0, LEN - cur_len]], constant_values=0.)])\n    return new_model, new_pad_model","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:02:36.186177Z","iopub.execute_input":"2021-12-21T02:02:36.186467Z","iopub.status.idle":"2021-12-21T02:02:36.217488Z","shell.execute_reply.started":"2021-12-21T02:02:36.186440Z","shell.execute_reply":"2021-12-21T02:02:36.216099Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"rets = []\n\npstart = np.zeros((in_ids.shape[0],LEN))\npend = np.zeros((in_ids.shape[0],LEN))\n\npSsum = np.zeros((in_ids_t.shape[0],LEN))\npEsum = np.zeros((in_ids_t.shape[0],LEN))\n\ndef doTraining():\n    for curFold, (idx1, idx2) in enumerate(strat.split(in_ids,train.sentiment.values)):\n        print(\"Fold \" + str(curFold+1) + \":\")\n        fname = \"fold\" + str(curFold+1) + \".h5\"\n\n        kfold.clear_session()\n        model, pad_model = build_model()\n\n        inp1 = [in_ids[idx1,], mask[idx1,], types[idx1,]]\n        target1 = [start[idx1,], end[idx1,]]\n\n        inp2 = [in_ids[idx2,], mask[idx2,], types[idx2,]]\n        target2 = [start[idx2,], end[idx2,]]\n\n        sortedData2 = np.int32(sorted(range(len(inp2[0])),\n                                   key=lambda i: (inp2[0][i] == 1).sum(),\n                                   reverse=True))\n\n        inp2 = [arr[sortedData2] for arr in inp2]\n        target2 = [arr[sortedData2] for arr in target2]\n\n        for epo in range(1, EPOCHS + 1):\n            sortedData1 = np.int32(sorted(range(len(inp1[0])),\n                                       key=lambda i: (inp1[0][i] == 1).sum() + np.random.randint(-3, 3),\n                                       reverse=True))\n            idxs = np.random.permutation(math.ceil(len(sortedData1) / BATCH_SIZE))\n            tmp = []\n            for idx in idxs:\n                tmp.append(sortedData1[idx * BATCH_SIZE: (idx + 1) * BATCH_SIZE])\n            sortedData1 = np.concatenate(tmp)\n\n            inp1 = [arr[sortedData1] for arr in inp1]\n            target1 = [arr[sortedData1] for arr in target1]\n\n            model.fit(inp1, target1, epochs=epo, initial_epoch=epo - 1,\n                      batch_size=BATCH_SIZE, verbose=1, callbacks=[],\n                      validation_data=(inp2, target2), shuffle=False)\n            dump_data(model, fname)\n\n        load_data(model, fname)\n        pstart[idx2,], pend[idx2,] = pad_model.predict([in_ids[idx2,], mask[idx2,], types[idx2,]], verbose=1)\n\n        pSE = pad_model.predict([in_ids_t, mask_t, types_t], verbose=1)\n        pSsum += pSE[0]/strat.n_splits\n        pEsum += pSE[1]/strat.n_splits\n\n        jacs = []\n        for idxCur in idx2:\n            maxS = np.argmax(pstart[idxCur,])\n            maxE = np.argmax(pend[idxCur,])\n            if maxS>maxE:\n                st = train.loc[idxCur,'text']\n            else:\n                enc = tknzer.encode(\" \"+\" \".join(train.loc[idxCur,'text'].split()))\n                st = tknzer.decode(enc.ids[maxS-2: maxE-1])\n            jacs.append(jaccard_idx(st, train.loc[idxCur, 'selected_text']))\n        rets.append(np.mean(jacs))\n        print(\"Fold \" + str(curFold+1) + \" Jaccard = \" + str(np.mean(jacs)) + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:02:36.219077Z","iopub.execute_input":"2021-12-21T02:02:36.219652Z","iopub.status.idle":"2021-12-21T02:02:36.247651Z","shell.execute_reply.started":"2021-12-21T02:02:36.219614Z","shell.execute_reply":"2021-12-21T02:02:36.246858Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"convertTrainData()\nconvertTestData()\ndoTraining()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:02:36.249064Z","iopub.execute_input":"2021-12-21T02:02:36.249477Z","iopub.status.idle":"2021-12-21T02:08:59.573572Z","shell.execute_reply.started":"2021-12-21T02:02:36.249439Z","shell.execute_reply":"2021-12-21T02:08:59.571934Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(rets))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:59.575016Z","iopub.status.idle":"2021-12-21T02:08:59.575848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rets) # Jaccard CVs","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:59.577107Z","iopub.status.idle":"2021-12-21T02:08:59.577924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all = []\n# for k in range(in_ids.shape[0]):\n#     a = np.argmax(pSsum[k,])\n#     b = np.argmax(pEsum[k,])\n#     if a>b:\n#         st = test.loc[k,'text']\n#     else:\n#         text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n#         enc = tknzer.encode(text1)\n#         st = tknzer.decode(enc.ids[a-2:b-1])\n#     all.append(st)\n#\n# test['selected_text'] = all\n# test[['textID','selected_text']].to_csv('submission.csv',index=False)\n# pd.set_option('max_colwidth', 60)\n# test.sample(25)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T02:08:59.579129Z","iopub.status.idle":"2021-12-21T02:08:59.579890Z"},"trusted":true},"execution_count":null,"outputs":[]}]}